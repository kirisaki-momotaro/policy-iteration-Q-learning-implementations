{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHoQfHWOvNh4",
        "outputId": "48918fa5-646f-4a3b-85b5-1067dcc6939b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cached Items: [47, 4, 19, 43, 9, 7, 6, 1, 18, 8]\n",
            "5 % done, cost: -0.6705140862449153\n",
            "10 % done, cost: -0.6314774228457644\n",
            "15 % done, cost: -0.5691809816236134\n"
          ]
        }
      ],
      "source": [
        "# Part 1. Network Friendly Recommendations Project\n",
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#CONTENT CATALOGUE\n",
        "#number of content items\n",
        "K=50\n",
        "#K = 2200\n",
        "\n",
        "#content relativity matrix\n",
        "u_ij = np.random.rand(K, K)\n",
        "\n",
        "#percentage of cached items\n",
        "C=0.2*K\n",
        "C=int(C)\n",
        "\n",
        "#cache\n",
        "cache=[0]*C\n",
        "\n",
        "#content relevance threshold\n",
        "u_min=0.4\n",
        "\n",
        "#USER MODEL\n",
        "#number of recomendations\n",
        "N=2\n",
        "#probability of ending viewing session\n",
        "q=0.01\n",
        "#probability of picking one item with equal prob. in case all are relevant\n",
        "alpha=0.9\n",
        "#probability when the user chooses from K items\n",
        "p_k=1/K\n",
        "\n",
        "# equates each action set to an integer\n",
        "actions=[]\n",
        "\n",
        "rewards=[]\n",
        "\n",
        "# function that initializes the cache\n",
        "def initialize_cache():\n",
        "   items_in_cache = random.sample(list(range(K)), C)\n",
        "   for i in range(len(items_in_cache)):\n",
        "    cache[i]=items_in_cache[i]\n",
        "\n",
        "# function that initializes the relativity matrix u_ij\n",
        "def initialize_relativity_matrix():\n",
        "  # set the diagonal elements of the array to 0\n",
        "  for i in range(K):\n",
        "    for j in range(0, i):\n",
        "        u_ij[i][j] = u_ij[j][i]\n",
        "    u_ij[i][i] = 0\n",
        "\n",
        "# function used to determine the cost of the item\n",
        "def item_cost(item):\n",
        "  if item in cache:\n",
        "    return 0\n",
        "  else:\n",
        "    return -1\n",
        "\n",
        "#check if all items are above u_min\n",
        "def all_bigger_than_u_min(current_item,recommended_items):\n",
        "  for i in recommended_items:\n",
        "    relevance=u_ij[i,current_item]\n",
        "    if(relevance<u_min):\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "#simulate user behaviour\n",
        "def user_sim(current_state,recommended_items):\n",
        "  done=False # Flag that indicates if a session is ended\n",
        "  new_state=-1\n",
        "  reward=0\n",
        "\n",
        "  end_session=np.random.rand()\n",
        "  #check if the user quits watching stupid memes and decides to do something better with their lives\n",
        "  if(end_session<q):\n",
        "    done=True\n",
        "  else: #user continues procrastinating instead of doing RL project\n",
        "    #check if all recomended items are relevant\n",
        "    if all_bigger_than_u_min(current_state,recommended_items):\n",
        "        p=np.random.rand()\n",
        "        # if p < a user picks one of the recommended items\n",
        "        if (p<alpha):\n",
        "          new_state = random.choice(recommended_items)\n",
        "          # check if the current state is the same as the new one\n",
        "          while new_state == current_state:\n",
        "            new_state = random.choice(recommended_items)\n",
        "        # if not they pick any item from the catalogue\n",
        "        else:\n",
        "          possible_states=list(range(0,K))\n",
        "          possible_states.remove(current_state)\n",
        "          new_state = random.choice(possible_states)\n",
        "\n",
        "    else:\n",
        "        possible_states=list(range(0,K))\n",
        "        possible_states.remove(current_state)\n",
        "        new_state = random.choice(possible_states)\n",
        "\n",
        "\n",
        "    reward=item_cost(new_state)\n",
        "  #print(new_state , reward , done)\n",
        "  return new_state , reward , done\n",
        "\n",
        "def initialize_actions():\n",
        "  items = list(range(K))\n",
        "  recommended_combinations = list(itertools.combinations(items, N))\n",
        "\n",
        "  for item_set in recommended_combinations:\n",
        "    actions.append(item_set)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "initialize_relativity_matrix()\n",
        "initialize_cache()\n",
        "initialize_actions()\n",
        "# Print the content catalogue\n",
        "#print(\"Content Catalogue:\")\n",
        "#print(u_ij)\n",
        "print(\"\\nCached Items:\", cache)\n",
        "#print(actions)\n",
        "\n",
        "# Initialize Q-table\n",
        "states = K  # Number of possible states\n",
        "K_minus=K-1 #it cant transition from e.g state 9 again to state 9\n",
        "actions_num = len(actions) # Number of possible actions\n",
        "Q_table = np.zeros((states, actions_num))  # init Q-table\n",
        "\n",
        "\n",
        "#encode each set of actions to a number in order to be compatible with the matrix\n",
        "\n",
        "gamma = 0.5 # discount factor\n",
        "\n",
        "# Q-learning algorithm\n",
        "def Q_learning_algorithm(max_episodes, gamma):\n",
        "  learning_rate = 0.1\n",
        "  # for tests:\n",
        "  # learning_rate = 0.3\n",
        "  # learning_rate = 0.7\n",
        "  #gamma = 0.5 # discount factor\n",
        "  exploration_rate = 1\n",
        "  max_exploration_rate=1\n",
        "  min_exploration_rate=0.01\n",
        "  exploration_decay_rate=0.01\n",
        "\n",
        "\n",
        "  for i in range(max_episodes):\n",
        "    state = random.choice(list(range(0,K))) # initialize start state\n",
        "    avg_reward = 0\n",
        "    num_of_watched_items = 0\n",
        "    while True:\n",
        "      # Choose an action using epsilon-greedy policy\n",
        "      if np.random.random() < exploration_rate:\n",
        "        action = np.random.randint(len(actions))\n",
        "        while state in actions[action]:\n",
        "          action = np.random.randint(len(actions))\n",
        "      else:\n",
        "        action = np.argmax(Q_table[state])\n",
        "        while state in actions[action]:\n",
        "          Q_table[state,action]=float('-inf')\n",
        "          action = np.argmax(Q_table[state])\n",
        "\n",
        "      next_state,reward,done=user_sim(state,actions[action])\n",
        "\n",
        "\n",
        "      # Update the Q-table\n",
        "      Q_table[state, action] = (1 - learning_rate) * Q_table[state, action] + learning_rate * (reward + gamma * np.max(Q_table[next_state, :]))\n",
        "\n",
        "      # update values\n",
        "      num_of_watched_items+=1\n",
        "      state=next_state\n",
        "      avg_reward = avg_reward + reward\n",
        "\n",
        "      exploration_rate = min_exploration_rate + \\\n",
        "      (max_exploration_rate-min_exploration_rate)*np.exp(-exploration_decay_rate*i)\n",
        "      # Check if session ends\n",
        "      if done:\n",
        "        rewards.append(avg_reward/num_of_watched_items)\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# run the algorithm\n",
        "gamma = 0.5\n",
        "dicrete_rewards=[]\n",
        "gamma0 = 0.1\n",
        "dicrete_rewards1=[]\n",
        "gamma2 = 0.9\n",
        "dicrete_rewards2=[]\n",
        "\n",
        "# run it for 3 gamma values\n",
        "for i in range(20):\n",
        "  avg_cost = Q_learning_algorithm(1000, gamma)\n",
        "  dicrete_rewards.append(sum(rewards)/len(rewards))\n",
        "  print(i*5+5,'% done, cost:',sum(rewards)/len(rewards)) # used to observe how fast it is running\n",
        "  rewards.clear()\n",
        "\n",
        "for i in range(20):\n",
        "  Q_table = np.zeros((states, actions_num))\n",
        "  avg_cost = Q_learning_algorithm(1000, gamma0)\n",
        "  dicrete_rewards1.append(sum(rewards)/len(rewards))\n",
        "  print(i*5+5,'% done, cost:',sum(rewards)/len(rewards)) # used to observe how fast it is running\n",
        "  rewards.clear()\n",
        "\n",
        "for i in range(20):\n",
        "  Q_table = np.zeros((states, actions_num))\n",
        "  avg_cost = Q_learning_algorithm(1000, gamma2)\n",
        "  dicrete_rewards2.append(sum(rewards)/len(rewards))\n",
        "  print(i*5+5,'% done, cost:',sum(rewards)/len(rewards)) # used to observe how fast it is running\n",
        "  rewards.clear()\n",
        "\n",
        "\n",
        "# Create the plot\n",
        "plt.plot(dicrete_rewards)\n",
        "plt.plot(dicrete_rewards, label='gamma = 0.5')\n",
        "plt.plot(dicrete_rewards1, color='r', label='gamma = 0.1')\n",
        "plt.plot(dicrete_rewards2, color='g', label='gamma = 0.9')\n",
        "plt.legend()\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('')\n",
        "\n",
        "plt.title(\"cost\")\n",
        "\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KkSw3sqaB7av"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}